---
name: Browser-Based Embeddings with Transformers.js
status: complete
created: 2025-10-14T03:28:21Z
updated: 2025-10-14T05:15:00Z
completed: 2025-10-14T05:15:00Z
github: https://github.com/tgarrochinho/lumara-frontend/issues/26
depends_on: []
parallel: true
conflicts_with: []
---

# Task: Browser-Based Embeddings with Transformers.js

## Description
Integrate Transformers.js to generate semantic embeddings in the browser using the MiniLM-L6-v2 model. Implement lazy loading, caching, and optimize for performance. This enables semantic similarity detection for contradiction and duplication detection without server costs.

## Acceptance Criteria
- [ ] @xenova/transformers package installed and configured
- [ ] MiniLM-L6-v2 model loads on first use and caches in browser
- [ ] generateEmbedding() function returns 384-dimensional vectors
- [ ] Embedding generation takes <100ms after initial model load
- [ ] Model caches persist across browser sessions
- [ ] Progress indicator during initial model download
- [ ] Error handling for model download failures with retry logic
- [ ] Memory usage stays under 200MB

## Technical Details

### Package Installation
```bash
npm install @xenova/transformers
```

### Files to Create
```
src/lib/ai/embeddings/
├── transformers.ts          # Main Transformers.js wrapper
├── cache.ts                 # Cache management utilities
└── types.ts                 # Embedding-related types

src/lib/ai/utils/
└── progress.ts              # Download progress tracking
```

### Key Implementation Steps

1. **Create Embeddings Service** (`src/lib/ai/embeddings/transformers.ts`)
```typescript
import { pipeline, env } from '@xenova/transformers';

// Configure Transformers.js
env.allowLocalModels = false; // Use CDN
env.allowRemoteModels = true;

class TransformersEmbeddingService {
  private embedder: any = null;
  private loading: boolean = false;
  private loadPromise: Promise<any> | null = null;

  async initialize(onProgress?: (progress: number) => void): Promise<void> {
    if (this.embedder) return;
    if (this.loadPromise) return this.loadPromise;

    this.loading = true;
    this.loadPromise = this.loadModel(onProgress);

    try {
      this.embedder = await this.loadPromise;
      this.loading = false;
    } catch (error) {
      this.loading = false;
      this.loadPromise = null;
      throw error;
    }
  }

  private async loadModel(onProgress?: (progress: number) => void): Promise<any> {
    const model = await pipeline(
      'feature-extraction',
      'Xenova/all-MiniLM-L6-v2',
      {
        progress_callback: (data: any) => {
          if (data.status === 'progress' && onProgress) {
            const progress = (data.loaded / data.total) * 100;
            onProgress(progress);
          }
        },
      }
    );

    return model;
  }

  async generateEmbedding(text: string): Promise<number[]> {
    if (!this.embedder) {
      await this.initialize();
    }

    const output = await this.embedder(text, {
      pooling: 'mean',
      normalize: true,
    });

    return Array.from(output.data);
  }

  async generateBatchEmbeddings(texts: string[]): Promise<number[][]> {
    if (!this.embedder) {
      await this.initialize();
    }

    const embeddings = await Promise.all(
      texts.map(text => this.generateEmbedding(text))
    );

    return embeddings;
  }

  isReady(): boolean {
    return this.embedder !== null && !this.loading;
  }

  isLoading(): boolean {
    return this.loading;
  }

  dispose(): void {
    // Transformers.js handles cleanup automatically
    this.embedder = null;
    this.loadPromise = null;
  }
}

// Singleton instance
export const embeddingsService = new TransformersEmbeddingService();
```

2. **Create Cache Management** (`src/lib/ai/embeddings/cache.ts`)
```typescript
export interface EmbeddingCache {
  text: string;
  embedding: number[];
  timestamp: Date;
}

export class EmbeddingCacheManager {
  private cache = new Map<string, EmbeddingCache>();

  set(text: string, embedding: number[]): void {
    this.cache.set(text, {
      text,
      embedding,
      timestamp: new Date(),
    });
  }

  get(text: string): number[] | null {
    const cached = this.cache.get(text);
    return cached ? cached.embedding : null;
  }

  has(text: string): boolean {
    return this.cache.has(text);
  }

  clear(): void {
    this.cache.clear();
  }

  size(): number {
    return this.cache.size;
  }

  // Get cache statistics
  getStats() {
    return {
      size: this.cache.size,
      oldestEntry: this.getOldestEntry(),
      newestEntry: this.getNewestEntry(),
    };
  }

  private getOldestEntry(): Date | null {
    let oldest: Date | null = null;
    for (const entry of this.cache.values()) {
      if (!oldest || entry.timestamp < oldest) {
        oldest = entry.timestamp;
      }
    }
    return oldest;
  }

  private getNewestEntry(): Date | null {
    let newest: Date | null = null;
    for (const entry of this.cache.values()) {
      if (!newest || entry.timestamp > newest) {
        newest = entry.timestamp;
      }
    }
    return newest;
  }
}

export const embeddingCache = new EmbeddingCacheManager();
```

3. **Wrapper Function with Caching** (`src/lib/ai/embeddings/transformers.ts` - add to existing file)
```typescript
import { embeddingCache } from './cache';

export async function generateEmbedding(
  text: string,
  useCache = true
): Promise<number[]> {
  // Check cache first
  if (useCache) {
    const cached = embeddingCache.get(text);
    if (cached) {
      return cached;
    }
  }

  // Generate new embedding
  const embedding = await embeddingsService.generateEmbedding(text);

  // Cache result
  if (useCache) {
    embeddingCache.set(text, embedding);
  }

  return embedding;
}
```

4. **Progress Tracking Utilities** (`src/lib/ai/utils/progress.ts`)
```typescript
export type ProgressCallback = (progress: number, message?: string) => void;

export class ProgressTracker {
  private callbacks: ProgressCallback[] = [];

  subscribe(callback: ProgressCallback): () => void {
    this.callbacks.push(callback);
    return () => {
      this.callbacks = this.callbacks.filter(cb => cb !== callback);
    };
  }

  update(progress: number, message?: string): void {
    this.callbacks.forEach(cb => cb(progress, message));
  }

  complete(): void {
    this.update(100, 'Complete');
  }

  error(message: string): void {
    this.callbacks.forEach(cb => cb(-1, `Error: ${message}`));
  }
}

export const embeddingProgress = new ProgressTracker();
```

### Performance Optimization

1. **Lazy Loading:**
   - Don't load model until first embedding requested
   - Show clear loading state to user

2. **Caching Strategy:**
   - Memory cache for session (Map)
   - IndexedDB cache for persistent storage (Task 004)
   - Never regenerate embeddings (deterministic)

3. **Batch Processing:**
   - Support batch embedding generation
   - Process multiple texts in parallel

### Testing Strategy
```typescript
// __tests__/embeddings.test.ts
describe('Embeddings Service', () => {
  it('generates 384-dimensional embeddings', async () => {
    const embedding = await generateEmbedding('test text');
    expect(embedding).toHaveLength(384);
  });

  it('caches embeddings correctly', async () => {
    const text = 'cached text';
    const embedding1 = await generateEmbedding(text);
    const embedding2 = await generateEmbedding(text);
    expect(embedding1).toEqual(embedding2);
  });

  it('handles model loading errors gracefully', async () => {
    // Test error handling
  });
});
```

## Dependencies
- [ ] @xenova/transformers package available
- [ ] Browser supports WebAssembly (required by Transformers.js)
- [ ] Sufficient browser memory (recommend 8GB+ RAM)

## Effort Estimate
- **Size:** M (Medium)
- **Hours:** 10-14 hours
- **Parallel:** Yes - Independent from Task 001

## Definition of Done
- [ ] @xenova/transformers installed and working
- [ ] Model loads successfully with progress tracking
- [ ] Embeddings generate in <100ms (after initial load)
- [ ] Cache system working (memory-based)
- [ ] Batch processing implemented
- [ ] Error handling with retry logic
- [ ] Performance benchmarks met
- [ ] Unit tests passing (>80% coverage)
- [ ] Documentation for embedding API
