---
name: Performance Optimization & Caching
status: open
created: 2025-10-14T03:28:21Z
updated: 2025-10-14T03:44:38Z
github: https://github.com/tgarrochinho/lumara-frontend/issues/19
depends_on: [002, 003]
parallel: true
conflicts_with: []
---

# Task: Performance Optimization & Caching

## Description
Optimize AI system performance to meet target benchmarks: <100ms embeddings, <2s chat, <50ms similarity search. Implement aggressive caching strategies and optimize hot paths.

## Acceptance Criteria
- [ ] Embedding generation <100ms after initial load
- [ ] Chat responses <2 seconds average
- [ ] Similarity search <50ms for 1000 memories
- [ ] Model caching persists across sessions
- [ ] Memory usage stays under 500MB total
- [ ] No UI blocking during AI operations
- [ ] Performance benchmarks documented

## Technical Details

### Files to Create/Modify
```
src/lib/ai/
├── performance.ts           # Performance monitoring
└── cache-strategy.ts        # Advanced caching

src/lib/ai/embeddings/
└── worker.ts                # Web Worker for embeddings (optional)
```

### Optimization Strategies

1. **Model Caching** (`src/lib/ai/cache-strategy.ts`)
```typescript
export class ModelCacheManager {
  private static CACHE_NAME = 'lumara-ai-models';

  static async cacheModel(url: string, data: ArrayBuffer): Promise<void> {
    const cache = await caches.open(this.CACHE_NAME);
    const response = new Response(data);
    await cache.put(url, response);
  }

  static async getCachedModel(url: string): Promise<ArrayBuffer | null> {
    const cache = await caches.open(this.CACHE_NAME);
    const response = await cache.match(url);

    if (response) {
      return await response.arrayBuffer();
    }

    return null;
  }

  static async clearCache(): Promise<void> {
    await caches.delete(this.CACHE_NAME);
  }

  static async getCacheSize(): Promise<number> {
    const cache = await caches.open(this.CACHE_NAME);
    const keys = await cache.keys();
    let totalSize = 0;

    for (const request of keys) {
      const response = await cache.match(request);
      if (response) {
        const blob = await response.blob();
        totalSize += blob.size;
      }
    }

    return totalSize;
  }
}
```

2. **Performance Monitoring** (`src/lib/ai/performance.ts`)
```typescript
export class PerformanceMonitor {
  private metrics: Map<string, number[]> = new Map();

  measure<T>(name: string, fn: () => Promise<T>): Promise<T> {
    const start = performance.now();

    return fn().finally(() => {
      const duration = performance.now() - start;

      if (!this.metrics.has(name)) {
        this.metrics.set(name, []);
      }

      this.metrics.get(name)!.push(duration);

      // Keep last 100 measurements
      const measurements = this.metrics.get(name)!;
      if (measurements.length > 100) {
        measurements.shift();
      }
    });
  }

  getStats(name: string) {
    const measurements = this.metrics.get(name) || [];

    if (measurements.length === 0) {
      return null;
    }

    const sorted = [...measurements].sort((a, b) => a - b);

    return {
      count: measurements.length,
      min: sorted[0],
      max: sorted[sorted.length - 1],
      avg: measurements.reduce((a, b) => a + b, 0) / measurements.length,
      median: sorted[Math.floor(sorted.length / 2)],
      p95: sorted[Math.floor(sorted.length * 0.95)],
      p99: sorted[Math.floor(sorted.length * 0.99)],
    };
  }

  getAllStats() {
    const stats: Record<string, any> = {};

    for (const [name, _] of this.metrics) {
      stats[name] = this.getStats(name);
    }

    return stats;
  }

  clear() {
    this.metrics.clear();
  }
}

export const performanceMonitor = new PerformanceMonitor();
```

3. **Optimized Embedding Generation** (`src/lib/ai/embeddings/transformers.ts` - modify existing)
```typescript
// Add memoization
const embeddingMemo = new Map<string, Promise<number[]>>();

export async function generateEmbedding(
  text: string,
  useCache = true
): Promise<number[]> {
  // Check memory cache first
  if (useCache && embeddingMemo.has(text)) {
    return embeddingMemo.get(text)!;
  }

  // Start generation
  const promise = (async () => {
    // Check persistent cache
    const cached = embeddingCache.get(text);
    if (cached && useCache) {
      return cached;
    }

    // Generate new embedding with performance tracking
    const embedding = await performanceMonitor.measure(
      'embedding-generation',
      () => embeddingsService.generateEmbedding(text)
    );

    // Cache result
    if (useCache) {
      embeddingCache.set(text, embedding);
    }

    return embedding;
  })();

  // Memoize promise
  if (useCache) {
    embeddingMemo.set(text, promise);
  }

  try {
    return await promise;
  } finally {
    // Clean up memo after completion
    embeddingMemo.delete(text);
  }
}
```

4. **Batch Processing Optimization** (`src/lib/ai/embeddings/transformers.ts` - add)
```typescript
export async function generateBatchEmbeddings(
  texts: string[],
  batchSize = 10
): Promise<number[][]> {
  const results: number[][] = [];

  // Process in batches to avoid memory issues
  for (let i = 0; i < texts.length; i += batchSize) {
    const batch = texts.slice(i, i + batchSize);

    const batchResults = await Promise.all(
      batch.map(text => generateEmbedding(text))
    );

    results.push(...batchResults);
  }

  return results;
}
```

5. **Similarity Search Optimization** (`src/lib/ai/utils/similarity.ts` - modify)
```typescript
// Add early termination for top-k search
export async function findTopKSimilar(
  queryEmbedding: number[],
  memories: Array<{ id: string; content: string; embedding?: number[] }>,
  k: number = 10,
  threshold = 0.7
): Promise<SimilarityMatch[]> {
  // Use min-heap to keep top k results
  const topK: SimilarityMatch[] = [];

  for (const memory of memories) {
    if (!memory.embedding) continue;

    const similarity = cosineSimilarity(queryEmbedding, memory.embedding);

    if (similarity >= threshold) {
      topK.push({
        id: memory.id,
        similarity,
        content: memory.content,
      });

      // Keep only top k
      if (topK.length > k) {
        topK.sort((a, b) => b.similarity - a.similarity);
        topK.pop();
      }
    }
  }

  return topK.sort((a, b) => b.similarity - a.similarity);
}
```

6. **Web Worker (Optional)** (`src/lib/ai/embeddings/worker.ts`)
```typescript
// Use Web Worker to avoid blocking main thread
export class EmbeddingWorker {
  private worker: Worker | null = null;

  async initialize(): Promise<void> {
    this.worker = new Worker(
      new URL('./embedding-worker.ts', import.meta.url),
      { type: 'module' }
    );
  }

  async generateEmbedding(text: string): Promise<number[]> {
    if (!this.worker) {
      await this.initialize();
    }

    return new Promise((resolve, reject) => {
      const messageHandler = (e: MessageEvent) => {
        if (e.data.type === 'embedding-result') {
          this.worker!.removeEventListener('message', messageHandler);
          resolve(e.data.embedding);
        } else if (e.data.type === 'error') {
          this.worker!.removeEventListener('message', messageHandler);
          reject(new Error(e.data.message));
        }
      };

      this.worker!.addEventListener('message', messageHandler);
      this.worker!.postMessage({ type: 'generate-embedding', text });
    });
  }

  terminate(): void {
    this.worker?.terminate();
    this.worker = null;
  }
}
```

### Performance Targets

| Operation | Target | Current | Status |
|-----------|--------|---------|--------|
| First-time setup | <30s | TBD | ⏳ |
| Subsequent load | <1s | TBD | ⏳ |
| Embedding generation | <100ms | TBD | ⏳ |
| Chat response | <2s | TBD | ⏳ |
| Similarity search (1K) | <50ms | TBD | ⏳ |
| Memory usage | <500MB | TBD | ⏳ |

### Optimization Checklist

- [ ] Enable model caching in Cache API
- [ ] Implement memoization for embeddings
- [ ] Add batch processing for multiple embeddings
- [ ] Optimize similarity search with early termination
- [ ] Add performance monitoring
- [ ] Consider Web Worker for heavy computations
- [ ] Lazy load unused modules
- [ ] Preload critical resources

### Testing
```typescript
describe('Performance', () => {
  it('meets embedding generation target', async () => {
    // Warm up
    await generateEmbedding('warmup');

    const start = performance.now();
    await generateEmbedding('test');
    const duration = performance.now() - start;

    expect(duration).toBeLessThan(100);
  });

  it('meets similarity search target', async () => {
    const query = new Array(384).fill(0.5);
    const memories = Array.from({ length: 1000 }, (_, i) => ({
      id: `${i}`,
      content: `memory ${i}`,
      embedding: new Array(384).fill(Math.random()),
    }));

    const start = performance.now();
    await findSimilar(query, memories);
    const duration = performance.now() - start;

    expect(duration).toBeLessThan(50);
  });

  it('cache reduces subsequent load time', async () => {
    // First load
    const start1 = performance.now();
    await generateEmbedding('test');
    const duration1 = performance.now() - start1;

    // Second load (cached)
    const start2 = performance.now();
    await generateEmbedding('test');
    const duration2 = performance.now() - start2;

    expect(duration2).toBeLessThan(duration1 * 0.1); // 10x faster
  });
});
```

## Dependencies
- [ ] Tasks 002-003 complete for optimization targets
- [ ] Cache API supported (all modern browsers)

## Effort Estimate
- **Size:** M (Medium)
- **Hours:** 10-14 hours
- **Parallel:** Yes - Can work independently

## Definition of Done
- [ ] All performance targets met
- [ ] Caching strategies implemented
- [ ] Performance monitoring in place
- [ ] Benchmarks documented
- [ ] Optimization guide written
- [ ] Memory profiling completed
- [ ] No performance regressions
